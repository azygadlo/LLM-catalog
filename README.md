# LLM-catalog
Large Language Models summarized in a table

|model                |year|paper                      |model type / objective                     |short info                                                                                                                                |parameters  |training corpora                                                                                        |company               |original code                                                                                |
|---------------------|----|---------------------------|-------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------|------------|--------------------------------------------------------------------------------------------------------|----------------------|---------------------------------------------------------------------------------------------|
|-                    |2015|Dai et al.                 |RNN (LSTM)                                 |idea of fine-tuning pre-trained domain-specific language models                                                                           |-           |IMDB, DBPedia, Newsgroups                                                                               |                      |                                                                                             |
|Transformer          |2017|Vaswani et al.             |seq2seq (for machine translation; not a LM)|original Transformer architecture                                                                                                         |up to 213M  |WMT (translation dataset)                                                                               |Google                |                                                                                             |
|ULMFiT               |2018|Howard & Ruder             |RNN (AWD-LSTM)                             |idea of fine-tuning pre-trained general-domain language models                                                                            |            |Wikitext-103                                                                                            |-                     |                                                                                             |
|ELMo                 |2018|Peters et al.              |bidirectional RNN                          |embeddings from LM added as input to other task-specific models                                                                           |94M         |1B Word LM Benchmark                                                                                    |Allen Institute for AI|                                                                                             |
|GPT                  |2018|Radford et al.             |autoregressive                             |first LLM using Transformer model (decoder)                                                                                               |110M        |BooksCorpus                                                                                             |OpenAI                |https://github.com/openai/finetune-transformer-lm                                            |
|BERT                 |2018|Devlin et al.              |masked LM + next sentence prediction       |idea of masked language modeling (bidirectional encoder)                                                                                  |110M / 340M |BooksCorpus + Wikipedia                                                                                 |Google                |https://github.com/google-research/bert                                                      |
|Transformer-XL       |2019|Dai et al.                 |autoregressive                             |beyond fixed-length context (processing segments)                                                                                         |?           |Wikitext-103, 1B Word Benchmark                                                                         |CMU + Google          |https://github.com/kimiyoung/transformer-xl                                                  |
|XLM                  |2019|Lample & Conneau           |autoregressive or MLM                      |cross-lingual language models                                                                                                             |570M        |Wikipedia; MultiUN; OPUS                                                                                |Facebook              |https://github.com/facebookresearch/XLM                                                      |
|GPT-2                |2019|Radford et al.             |autoregressive                             |first model to surpass 1B parameters                                                                                                      |1.5B        |WebText (OpenAI internal), 40GB                                                                         |OpenAI                |https://github.com/openai/gpt-2                                                              |
|ERNIE                |2019|Zhang et al.               |masked LM + denoising autoencoder          |text encoder + knowledge graph                                                                                                            |114M        |Wikipedia + Wikidata                                                                                    |Tsinghua University   |https://github.com/thunlp/ERNIE                                                              |
|XLNet                |2019|Yang et al.                |permutation                                |idea of permutation language modeling                                                                                                     |340M        |BooksCorpus + Wikipedia + Giga5 + ClueWeb + CommonCrawl                                                 |CMU + Google          |https://github.com/zihangdai/xlnet                                                           |
|RoBERTa              |2019|Liu et al.                 |masked LM                                  |modifications to BERT after ablation study                                                                                                |355M        |BooksCorpus + Wikipedia + CC-News + OpenWebText + Stories, 160 GB                                       |Facebook              |https://github.com/facebookresearch/fairseq/tree/main/examples/roberta                       |
|Megatron-LM          |2019|Shoeybi et al.             |autoregressive or MLM                      |even larger multi-billion parameter models based on GPT / BERT                                                                            |8.3B        |Wikipedia + CC-Stories + RealNews + OpenWebText                                                         |NVIDIA                |-                                                                                            |
|ALBERT               |2019|Lan et al.                 |masked LM + sentence order prediction      |reduced #params: embedding decomposition + cross-layer param sharing                                                                      |up to 235M  |same as BERT                                                                                            |Google                |https://github.com/google-research/ALBERT                                                    |
|DistilBERT           |2019|Sanh et al.                |masked LM + next sentence prediction       |obtained from BERT via knowledge distillation (teacher-student)                                                                           |66M         |same as BERT                                                                                            |Hugging Face          |https://github.com/huggingface/transformers/tree/main/examples/research_projects/distillation|
|T5                   |2019|Raffel et al.              |seq2seq                                    |encoder-decoder pre-trained with unsupervised denoising objective, fine-tuned with multi-task objective (tasks formulated as text-to-text)|up to 11B   |C4 (Colossal Clean Crawled Corpus), 750GB (stage 1); supervised datasets (stage 2)                      |Google                |https://github.com/google-research/text-to-text-transfer-transformer                         |
|BART                 |2019|Lewis et al.               |seq2seq                                    |pre-trained as a denoising autoencoder: to restore the corrupted input                                                                    |BERT+10%    |same as BERT                                                                                            |Facebook              |https://github.com/facebookresearch/fairseq/tree/main/examples/bart                          |
|XLM-RoBERTa          |2019|Conneau et al.             |masked LM                                  |multi-lingual model pre-trained on texts in 100 languages                                                                                 |550M        |CommonCrawl in 100 languages                                                                            |Facebook              |https://github.com/facebookresearch/XLM                                                      |
|Meena                |2020|Adiwardana et al.          |seq2seq (for dialogue)                     |multi-turn chatbot trained to minimize perplexity of the next token                                                                       |2.6B        |public domain social media conversations                                                                |Google                |-                                                                                            |
|Turing NLG           |2020|only blogpost              |autoregressive                             |a model scaled up to 17B parameters                                                                                                       |17B         |same type of data that Megatron-LM models were trained on                                               |Microsoft             |-                                                                                            |
|ELECTRA              |2020|Clark et al.               |replaced token detection                   |GAN-like pre-training; generator corrupts the input, discriminator detects corrupted tokens                                               |same as BERT|same as BERT, for largest model: same as XLNet                                                          |Stanford + Google     |https://github.com/google-research/electra                                                   |
|GPT-3                |2020|Brown et al.               |autoregressive                             |very similar to GPT-2, but larger (175B params; largest to date)                                                                          |175B        |CommonCrawl + extended WebText + Books + Wikipedia                                                      |OpenAI                |OpenAI API (https://beta.openai.com/docs/models/gpt-3)                                       |
|DeBERTa              |2020|He et al.                  |masked LM                                  |BERT with disentangled attention (word content and position separated) + enhanced mask decoder                                            |up to 1.5B  |Wikipedia + BooksCorpus + OpenWebText + Stories                                                         |Microsoft             |https://github.com/microsoft/DeBERTa                                                         |
|mT5                  |2020|Xue et al.                 |seq2seq                                    |multilingual T5 for 101 languages                                                                                                         |up to 11B   |CommonCrawl in 101 languages (mC4)                                                                      |Google                |https://github.com/google-research/multilingual-t5                                           |
|Switch Transformer   |2021|Fedus et al.               |seq2seq (Mixture of Experts)               |sparsely-activated model / MoE - parameters (part of the model to be used) depend on the input data                                       |1.6T (MoE)  |same as in T5 and mT5                                                                                   |Google                |-                                                                                            |
|GLM                  |2021|Du et al.                  |                                           |                                                                                                                                          |130B        |                                                                                                        |Tsinghua University   |                                                                                             |
|GPT-Neo              |2021|-                          |autoregressive                             |replication of the GPT-3 architecture (with much less parameters)                                                                         |2.7B        |The Pile                                                                                                |EleutherAI            |https://github.com/EleutherAI/gpt-neo                                                        |
|GPT-J                |2021|-                          |autoregressive                             |replication of the GPT-3 architecture (with much less parameters); seems very similar to GPT-Neo                                          |6B          |The Pile                                                                                                |EleutherAI            |https://huggingface.co/EleutherAI/gpt-j-6B                                                   |
|Jurassic-1           |2021|Lieber et al.              |autoregressive                             |GPT-3 like with "optimized" depth-to-width ratio (shallower but wider?) and larger vocabulary                                             |178B        |attempt to replicate GPT-3 data using publicly available data                                           |AI21 Labs             |API (https://www.ai21.com/studio)                                                            |
|FLAN                 |2021|Wei et al.                 |autoregressive                             |137B LaMDA-PT model fine-tuned on instructions                                                                                            |137B        |a mixture of 62 NLU and NLG tasks (see paper for details)                                               |Google                |-                                                                                            |
|T0                   |2022|Sanh et al.                |seq2seq                                    |T5 model fine-tuned on a large mixture of supervised tasks with a unified prompt format                                                   |11B         |https://huggingface.co/datasets/bigscience/P3                                                           |Hugging Face          |https://github.com/bigscience-workshop/t-zero                                                |
|Megatron-Turing NLG  |2021|Smith et al.               |autoregressive                             |largest model at that time, 3x larger than GPT-3                                                                                          |530B        |a subset of The Pile + CommonCrawl + RealNews + CC-Stories                                              |Microsoft + NVIDIA    |-                                                                                            |
|RETRO                |2022|Borgeaud et al.            |seq2seq (+ retrieval)                      |input is split into chunks; for each chunk, nearest neighbor entries are retrieved from DB to improve modeling                            |up to 7B    |multilingual MassiveText (see Gopher paper)                                                             |DeepMind              |-                                                                                            |
|GLaM                 |2021|Du et al.                  |autoregressive (Mixture of Experts)        |another MoE model, this time autoregressive, with over a trillion parameters                                                              |1.3T (MoE)  |a mixture of webpages, conversations, forums, books, news                                               |Google                |-                                                                                            |
|Gopher               |2022|Rae et al.                 |autoregressive                             |a family of language models (up to 280B) plus analysis of effect of model scaling                                                         |up to 280B  |MassiveText (MassiveWeb + C4 + Books + News + Wiki + GitHub)                                            |DeepMind              |-                                                                                            |
|LaMDA                |2022|Thoppilan et al.           |autoregressive (for dialogue)              |pre-trained on public dialogues and web documents, fine-tuned for safety and factual correctness (knowledge retrieval from external tools)|137B        |publicly available dialogues and web documents (details in paper)                                       |Google                |-                                                                                            |
|ST-MoE               |2022|Zoph et al.                |seq2seq (Mixture of Experts)               |stable training of a large-scale sparse (Mixture of Experts) language model                                                               |269B (MoE)  |mix of C4 corpus and dataset used for GLaM                                                              |Google                |-                                                                                            |
|InstructGPT          |2022|Ouyang et al.              |autoregressive                             |GPT-3 model trained to follow instructions using Reinforcement Learning with Human Feedback (RLHF)                                        |175B        |human demonstrations of desired model behavior for prompts (manually written + collected via OpenAI API)|OpenAI                |OpenAI API (https://beta.openai.com/docs/models/gpt-3)                                       |
|Chinchilla           |2022|Hoffmann et al.            |autoregressive                             |compute-optimal training; 4x smaller than Gopher but trained on 4x more data,  beats larger models on many downstream tasks               |70B         |MassiveText (a different subset distribution than in Gopher)                                            |DeepMind              |-                                                                                            |
|PaLM                 |2022|Chowdhery et al.           |autoregressive                             |largest model to date, efficiently trained using Google Pathways system                                                                   |540B        |based on datasets used in GLaM and LaMDA                                                                |Google                |-                                                                                            |
|Anthropic assistant  |2022|Bai et al.                 |autoregressive (for dialogue)              |dialogue agent based on a language model trained with RLHF to be helpful and harmless                                                     |up to 52B   |The Pile                                                                                                |Anthropic             |-                                                                                            |
|GPT-NeoX             |2022|Black et al.               |autoregressive                             |largest publicly available dense autoregressive model to date (when released)                                                             |20B         |The Pile                                                                                                |EleutherAI            |https://github.com/EleutherAI/gpt-neox                                                       |
|OPT                  |2022|Zhang et al.               |autoregressive                             |a family of language models (up to 175B) that (apart from the largest one) have publicly available weights                                |up to 175B  |dataset from RoBERTa + The Pile + Reddit                                                                |Meta                  |https://github.com/facebookresearch/metaseq/tree/main/projects/OPT                           |
|YaLM                 |2022|-                          |                                           |                                                                                                                                          |100B        |                                                                                                        |Yandex                |https://github.com/yandex/YaLM-100B                                                          |
|Atlas                |2022|Izacard et al.             |seq2seq (+ retrieval)                      |T5 language model + retrieval from a corpus of documents (joint pretraining)                                                              |up to 11B   |Wikipedia, CommonCrawl                                                                                  |Meta                  |-                                                                                            |
|Sparrow              |2022|Glaese et al.              |autoregressive (for dialogue)              |dialogue agent based on Chinchilla LM trained with RLHF to be helpful and harmless, able to retrieve information from external source     |70B         |dialogue data collected by interaction with human annotators                                            |DeepMind              |-                                                                                            |
|Flan-T5 (+ Flan-PaLM)|2022|Chung et al.               |seq2seq / autoregressive                   |T5 and PaLM models fine-tuned with instructions (FLAN-T5 weights released in several sizes)                                               |up to 540B  |a mixture of 1836 finetuning tasks from 4 sources (details in paper)                                    |Google                |https://github.com/google-research/t5x                                                       |
|BLOOM                |2022|Le Scao et al. (BigScience)|autoregressive                             |a 176B parameter model resulting from the BigScience collaboration (trained for 3.5 months in the first half of the year)                 |176B        |ROOTS dataset (mix of natural and programming languages)                                                |BigScience            |https://github.com/bigscience-workshop/petals                                                |
|BLOOMZ               |2022|Muennighof et al.          |autoregressive                             |BLOOM finetuned on instructions                                                                                                           |176B        |                                                                                                        |                      |                                                                                             |
|Galactica            |2022|Taylor et al.              |autoregressive                             |a model trained on a corpus of scientific knowledge, performing strongly in knowledge-intensive scientific tasks                          |up to 120B  |papers, textbooks, encyclopedias, code, knowledge bases etc.                                            |Meta                  |-                                                                                            |
|ChatGPT              |2022|only blogpost (for now)    |autoregressive (for dialogue)              |a model trained in a similar way as InstructGPT, using RLHF, in a dialogue/chat framework                                                 |-           |-                                                                                                       |OpenAI                |https://chat.openai.com/                                                                     |
